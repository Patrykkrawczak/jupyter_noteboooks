{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## UCZENIE RĘKI ROBOTA PRZEPYCHANIA KRĄŻKA W OKREŚLONY PUNKT\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "import gym.wrappers as wr\n",
    "from gym import Wrapper\n",
    "import math\n",
    "import numpy as np\n",
    "from collections import deque\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "\n",
    "import torch\n",
    "torch.manual_seed(0) \n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.autograd import Variable\n",
    "from torch.distributions import Categorical\n",
    "\n",
    "\n",
    "from pyvirtualdisplay import Display\n",
    "display = Display(visible=0, size=(1400, 900))\n",
    "display.start()\n",
    "\n",
    "is_ipython = 'inline' in plt.get_backend()\n",
    "if is_ipython:\n",
    "    from IPython import display\n",
    "\n",
    "plt.ion()\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reset(env):\n",
    "    state = env.reset()\n",
    "    rrr = env.env.env\n",
    "    al = []\n",
    "    if rrr.has_object:\n",
    "            object_xpos = rrr.initial_gripper_xpos[:2]\n",
    "            object_qpos = rrr.sim.data.get_joint_qpos('object0:joint')\n",
    "            assert object_qpos.shape == (7,)\n",
    "            object_qpos[:2] = object_xpos\n",
    "            object_qpos[0] = 1.10\n",
    "            object_qpos[1] = 0.75\n",
    "            object_qpos[3] = 0\n",
    "            object_qpos[-2] = 0.6\n",
    "            rrr.sim.data.set_joint_qpos('object0:joint', object_qpos)\n",
    "            al = object_qpos[0:3]\n",
    "\n",
    "    env.env.env = rrr\n",
    "    env.env.env.goal = np.array([1.3, 0.70, 0.42186031])\n",
    "    state = np.concatenate(( np.array(state['observation']) ,np.array(al), np.array([1.3, 0.70, 0.42186031])))\n",
    "\n",
    "    \n",
    "    return state\n",
    "\n",
    "def step(env,action):\n",
    "    state, reward, done, a = env.step(tuple(action))\n",
    "    reward = -1*((state [\"desired_goal\"][0] - state [\"achieved_goal\"][0])**2 +(state [\"desired_goal\"][1] - state [\"achieved_goal\"][1])**2 +(state [\"desired_goal\"][2] - state [\"achieved_goal\"][2])**2 )\n",
    "\n",
    "    if a[\"is_success\"] > 0:\n",
    "        reward+=3000\n",
    "\n",
    "    state = np.concatenate(( np.array(state['observation']) ,np.array(state[\"achieved_goal\"]), np.array(state[\"desired_goal\"])))\n",
    "    return state, reward, done, a\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0.99570627  0.74890684  0.42185313  0.70644302 -0.49749877  0.50338356\n",
      "  0.00618184]\n"
     ]
    }
   ],
   "source": [
    "# Przygotowanie środowiska \n",
    "env = gym.make('FetchSlide-v1')\n",
    "\n",
    "env.env.reward_type=\"dense\"\n",
    "\n",
    "obs = env.reset()\n",
    "rrr = env.env\n",
    "al = []\n",
    "if rrr.has_object:\n",
    "        object_xpos = rrr.initial_gripper_xpos[:2]\n",
    "        object_qpos = rrr.sim.data.get_joint_qpos('object0:joint')\n",
    "        assert object_qpos.shape == (7,)\n",
    "        object_qpos[:2] = object_xpos\n",
    "        print(object_qpos)\n",
    "        object_qpos[0] = 1.10\n",
    "        object_qpos[1] = 0.75\n",
    "        object_qpos[3] = 0\n",
    "        object_qpos[-2] = 0.6\n",
    "        rrr.sim.data.set_joint_qpos('object0:joint', object_qpos)\n",
    "        al = object_qpos[0:3]\n",
    "\n",
    "env.env = rrr\n",
    "env.env.goal = np.array([1.8, 0.75, 0.42186031])\n",
    "\n",
    "env = wr.Monitor(env, '/media/patryk/Inne_pliki/Pobrane/', force = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Agent(nn.Module):\n",
    "    def __init__(self,env,input_size = 31,hidden_size = 310,output_size = 4):\n",
    "        super(Agent, self).__init__()\n",
    "        \n",
    "        self.env = env\n",
    "        self.i_s = input_size\n",
    "        \n",
    "        self.h_s = hidden_size\n",
    "        self.o_s = output_size \n",
    "        \n",
    "        self.fc1 = nn.Linear(self.i_s, self.h_s)\n",
    "        self.fc2 = nn.Linear(self.h_s, self.o_s)\n",
    "        \n",
    "    def ustaw_wagi(self,wagi):\n",
    "        h = self.h_s\n",
    "        i = self.i_s\n",
    "        o = self.o_s\n",
    "        fc1_rozm = (i*h)+h\n",
    "        \n",
    "        fc1_W = torch.from_numpy(wagi[:i*h].reshape(i, h)) #Wagi 1 warstwa\n",
    "        fc1_b = torch.from_numpy(wagi[i*h:fc1_rozm]) #Bias 1 warstwa\n",
    "        fc2_W = torch.from_numpy(wagi[fc1_rozm:fc1_rozm+(h*o)].reshape(h, o)) #Wagi 2 warstwa\n",
    "        fc2_b = torch.from_numpy(wagi[fc1_rozm+(h*o):]) #Bias 2 warstwa\n",
    "        \n",
    "        self.fc1.weight.data.copy_(fc1_W.view_as(self.fc1.weight.data))\n",
    "        self.fc1.bias.data.copy_(fc1_b.view_as(self.fc1.bias.data))\n",
    "        self.fc2.weight.data.copy_(fc2_W.view_as(self.fc2.weight.data))\n",
    "        self.fc2.bias.data.copy_(fc2_b.view_as(self.fc2.bias.data))\n",
    "        \n",
    "    \n",
    "    def get_wagi(self):\n",
    "        return self.i_s*self.h_s+ self.h_s + self.h_s*self.o_s + self.o_s\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = torch.tanh(self.fc2(x))\n",
    "        return x.cpu().data\n",
    "        \n",
    "    def evaluate(self, wagi, gamma=1.0, max_t=5000):\n",
    "        self.ustaw_wagi(wagi)\n",
    "        episode_return = 0.0\n",
    "        state = reset(self.env)\n",
    "    \n",
    "        for t in range(max_t):\n",
    "            state = torch.from_numpy(state).float().to(device)\n",
    "            action = self.forward(state)\n",
    "            state, reward, done, a = step(env, (tuple(action)))\n",
    "            episode_return += reward * math.pow(gamma, t)\n",
    "            if done:\n",
    "                break\n",
    "        return episode_return\n",
    "    \n",
    "agent = Agent(env)      \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episod: 10\tŚredni wynik: -1.61\n",
      "Episod: 20\tŚredni wynik: 2698.95\n",
      "Episod: 30\tŚredni wynik: 1799.13\n",
      "Episod: 40\tŚredni wynik: 1349.23\n",
      "Episod: 50\tŚredni wynik: 1079.28\n",
      "Episod: 60\tŚredni wynik: 899.32\n",
      "Episod: 70\tŚredni wynik: 770.78\n",
      "Episod: 80\tŚredni wynik: 674.37\n",
      "Episod: 90\tŚredni wynik: 899.39\n",
      "Episod: 100\tŚredni wynik: 809.40\n",
      "Episod: 110\tŚredni wynik: 1649.51\n",
      "Episod: 120\tŚredni wynik: 1379.34\n",
      "Episod: 130\tŚredni wynik: 1679.02\n",
      "Episod: 140\tŚredni wynik: 2818.59\n",
      "Episod: 150\tŚredni wynik: 3118.28\n",
      "Episod: 160\tŚredni wynik: 3328.15\n",
      "Episod: 170\tŚredni wynik: 3328.15\n",
      "Episod: 180\tŚredni wynik: 3328.15\n",
      "Episod: 190\tŚredni wynik: 3058.14\n",
      "Episod: 200\tŚredni wynik: 3058.15\n",
      "Episod: 210\tŚredni wynik: 3148.15\n",
      "Episod: 220\tŚredni wynik: 2878.32\n",
      "Episod: 230\tŚredni wynik: 3058.64\n",
      "Episod: 240\tŚredni wynik: 1919.06\n",
      "Episod: 250\tŚredni wynik: 1619.38\n",
      "Episod: 260\tŚredni wynik: 1829.51\n",
      "Episod: 270\tŚredni wynik: 1829.51\n",
      "Episod: 280\tŚredni wynik: 1829.51\n",
      "Episod: 290\tŚredni wynik: 1829.51\n",
      "Episod: 300\tŚredni wynik: 1829.51\n",
      "Episod: 310\tŚredni wynik: 899.51\n",
      "Episod: 320\tŚredni wynik: 1349.51\n",
      "Episod: 330\tŚredni wynik: 869.51\n",
      "Episod: 340\tŚredni wynik: 869.51\n",
      "Episod: 350\tŚredni wynik: 869.51\n",
      "Episod: 360\tŚredni wynik: 959.51\n",
      "Episod: 370\tŚredni wynik: 1409.51\n",
      "Episod: 380\tŚredni wynik: 1409.51\n",
      "Episod: 390\tŚredni wynik: 1739.51\n",
      "Episod: 400\tŚredni wynik: 1979.51\n",
      "Episod: 410\tŚredni wynik: 2339.51\n",
      "Episod: 420\tŚredni wynik: 3029.51\n",
      "Episod: 430\tŚredni wynik: 3389.51\n",
      "Episod: 440\tŚredni wynik: 3899.51\n",
      "Episod: 450\tŚredni wynik: 3899.51\n",
      "Episod: 460\tŚredni wynik: 3689.51\n",
      "Episod: 470\tŚredni wynik: 3809.51\n",
      "Episod: 480\tŚredni wynik: 4349.51\n",
      "Episod: 490\tŚredni wynik: 4649.51\n",
      "Episod: 500\tŚredni wynik: 4739.51\n",
      "Episod: 510\tŚredni wynik: 4379.52\n",
      "Episod: 520\tŚredni wynik: 3239.51\n",
      "Episod: 530\tŚredni wynik: 3509.52\n",
      "Episod: 540\tŚredni wynik: 2999.51\n",
      "Episod: 550\tŚredni wynik: 3929.52\n",
      "Episod: 560\tŚredni wynik: 3629.51\n",
      "Episod: 570\tŚredni wynik: 3059.51\n",
      "Episod: 580\tŚredni wynik: 3149.51\n",
      "Episod: 590\tŚredni wynik: 2519.51\n",
      "Episod: 600\tŚredni wynik: 2459.51\n",
      "Episod: 610\tŚredni wynik: 2459.51\n",
      "Episod: 620\tŚredni wynik: 2999.51\n",
      "Episod: 630\tŚredni wynik: 2699.51\n",
      "Episod: 640\tŚredni wynik: 2939.51\n",
      "Episod: 650\tŚredni wynik: 2519.51\n",
      "Episod: 660\tŚredni wynik: 4169.51\n",
      "Episod: 670\tŚredni wynik: 4169.51\n",
      "Episod: 680\tŚredni wynik: 4559.51\n",
      "Episod: 690\tŚredni wynik: 4829.51\n",
      "Episod: 700\tŚredni wynik: 4559.51\n",
      "Episod: 710\tŚredni wynik: 4949.51\n",
      "Episod: 720\tŚredni wynik: 5039.51\n",
      "Episod: 730\tŚredni wynik: 5279.51\n",
      "Episod: 740\tŚredni wynik: 5099.52\n",
      "Episod: 750\tŚredni wynik: 5249.52\n",
      "Episod: 760\tŚredni wynik: 4109.52\n",
      "Episod: 770\tŚredni wynik: 4109.52\n",
      "Episod: 780\tŚredni wynik: 3089.51\n",
      "Episod: 790\tŚredni wynik: 3059.52\n",
      "Episod: 800\tŚredni wynik: 3059.52\n",
      "Episod: 810\tŚredni wynik: 3059.52\n",
      "Episod: 820\tŚredni wynik: 2429.51\n",
      "Episod: 830\tŚredni wynik: 2339.52\n"
     ]
    }
   ],
   "source": [
    "def cem(N=5000, max_t=1000, gamma=1.0, print_for=10, populacja=30, ile_naj=0.25, sigma=0.3):\n",
    "    n_elite=int(populacja*ile_naj)\n",
    "\n",
    "    kolejka = deque(maxlen=100)\n",
    "    wyniki = []\n",
    "    najlepszy = sigma*np.random.randn(agent.get_wagi())\n",
    "\n",
    "    for iteracja in range(1, N+1):\n",
    "        populacja_nowych = [najlepszy + ((sigma**i+sigma)*np.random.randn(agent.get_wagi())) for i in range(populacja)]\n",
    "        tab = np.array([agent.evaluate(wynikowa, gamma, max_t) for wynikowa in populacja_nowych])\n",
    "        \n",
    "        indeksy = tab.argsort()[-n_elite:]\n",
    "        najlepsze_wagi = [populacja_nowych[i] for i in indeksy]\n",
    "        najlepszy = np.array(najlepsze_wagi).mean(axis=0)\n",
    "\n",
    "        wynik = agent.evaluate(najlepszy, gamma=1.0)\n",
    "        kolejka.append(wynik)\n",
    "        wyniki.append(wynik)\n",
    "        \n",
    "        torch.save(agent.state_dict(), 'checkpoint.pth')\n",
    "        \n",
    "        if iteracja % print_for == 0:\n",
    "            print('Episod: {}\\tŚredni wynik: {:.2f}'.format(iteracja, np.mean(kolejka)))\n",
    "\n",
    "    return wyniki\n",
    "\n",
    "scores = cem()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(111)\n",
    "plt.plot(np.arange(1, len(scores)+1), scores)\n",
    "plt.ylabel('WYNIK')\n",
    "plt.xlabel('EPISOD')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "img = plt.imshow(env.render(mode='rgb_array'))\n",
    "state = reset(env)\n",
    "while True:\n",
    "    state = torch.from_numpy(state).float().to(device)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        action = agent(state)\n",
    "    img.set_data(env.render(mode='rgb_array')) \n",
    "    plt.axis('off')\n",
    "    display.display(plt.gcf())\n",
    "    display.clear_output(wait=True)\n",
    "    state, reward, done, a = step(env,tuple(action))\n",
    "    if done:\n",
    "        break\n",
    "\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
